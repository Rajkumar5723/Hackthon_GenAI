{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Conda\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Conda\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/679 [00:00<?, ?it/s]d:\\Conda\\envs\\myenv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 679/679 [2:23:55<00:00, 12.72s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.01727800669655374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [2:18:17<00:00, 12.22s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0007954373385994497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [2:17:56<00:00, 12.19s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.00035152179068153304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [2:17:47<00:00, 12.18s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.00019806015780312182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [2:17:45<00:00, 12.17s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.00012414911306248526\n",
      "Validation accuracy: 2.00\n",
      "Test accuracy: 2.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(r'D:\\Intel\\sentimental\\goemotions_encoded_train.csv')\n",
    "val_data = pd.read_csv(r'D:\\Intel\\sentimental\\goemotions_encoded_validation.csv')\n",
    "test_data = pd.read_csv(r'D:\\Intel\\sentimental\\goemotions_encoded_test.csv')\n",
    "\n",
    "# Preprocessing\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokenizer.encode(text, max_length=100, truncation=True, padding='max_length')),\n",
    "            'attention_mask': torch.tensor([1] * len(tokenizer.encode(text, max_length=100, truncation=True, padding='max_length'))),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Prepare labels (the remaining columns are the labels)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(train_data.drop(columns=['text']).values)\n",
    "y_val = mlb.transform(val_data.drop(columns=['text']).values)\n",
    "y_test = mlb.transform(test_data.drop(columns=['text']).values)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(train_data['text'].values, y_train)\n",
    "val_dataset = CustomDataset(val_data['text'].values, y_val)\n",
    "test_dataset = CustomDataset(test_data['text'].values, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=y_train.shape[1])\n",
    "model.to(device)\n",
    "\n",
    "# Training the model\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.sigmoid(outputs.logits)  # Sigmoid for multi-label\n",
    "        total_accuracy += ((predictions > 0.5) == labels).float().sum().item()\n",
    "\n",
    "val_accuracy = total_accuracy / len(val_dataset)\n",
    "print(f'Validation accuracy: {val_accuracy:.2f}')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "model.eval()\n",
    "total_test_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.sigmoid(outputs.logits)\n",
    "        total_test_accuracy += ((predictions > 0.5) == labels).float().sum().item()\n",
    "test_accuracy = total_test_accuracy / len(test_dataset)\n",
    "print(f'Test accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to D:/Intel/sentimental/saved_roberta_model\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory to save the model and tokenizer\n",
    "save_directory = r'D:/Intel/sentimental/saved_roberta_model'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f'Model and tokenizer saved to {save_directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_directory = r'D:/Intel/sentimental/saved_roberta_model'\n",
    "loaded_model = RobertaForSequenceClassification.from_pretrained(model_directory)\n",
    "loaded_tokenizer = RobertaTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Function to predict emotions from text input\n",
    "def predict_emotions(text, model, tokenizer, threshold=0.5):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=100).to(device)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert logits to probabilities using sigmoid\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    \n",
    "    # Emotion labels based on your dataset\n",
    "    emotions = [\n",
    "        'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire',\n",
    "        'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "        'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', \n",
    "        'sadness', 'surprise'\n",
    "    ]\n",
    "    \n",
    "    # Apply threshold to decide which emotions are present\n",
    "    predicted_labels = (probabilities > threshold).cpu().numpy()[0]\n",
    "    predicted_emotions = [emotions[i] for i in range(len(predicted_labels)) if predicted_labels[i] == 1]\n",
    "    \n",
    "    return predicted_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Why did you do that? I'm really angry and confused.\n",
      "Predicted emotions: ['admiration', 'amusement']\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_directory = r'D:/Intel/sentimental/saved_roberta_model'\n",
    "loaded_model = RobertaForSequenceClassification.from_pretrained(model_directory).to(device)\n",
    "loaded_tokenizer = RobertaTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "# Function to predict emotions\n",
    "def predict_emotions(text, model, tokenizer, threshold=0.5):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=100).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    \n",
    "    # List of emotions in the dataset\n",
    "    emotions = [\n",
    "        'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire',\n",
    "        'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "        'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', \n",
    "        'sadness', 'surprise'\n",
    "    ]\n",
    "    \n",
    "    # Get predicted labels\n",
    "    predicted_labels = (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    predicted_emotions = [emotions[i] for i in range(len(predicted_labels)) if predicted_labels[i] == 1]\n",
    "    \n",
    "    return predicted_emotions\n",
    "\n",
    "# Example input and output\n",
    "input_text = \"Why did you do that? I'm really angry and confused.\"\n",
    "predicted_emotions = predict_emotions(input_text, loaded_model, loaded_tokenizer)\n",
    "print(f'Input: {input_text}')\n",
    "print(f'Predicted emotions: {predicted_emotions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I'm so happy and excited to see you.\n",
      "Predicted emotions: ['admiration', 'amusement']\n",
      "--------------------------------------------------\n",
      "Input: Why did you do that? I'm really angry and confused.\n",
      "Predicted emotions: ['admiration', 'amusement']\n",
      "--------------------------------------------------\n",
      "Input: I can't believe this is happening. I'm so disappointed.\n",
      "Predicted emotions: ['admiration', 'amusement']\n",
      "--------------------------------------------------\n",
      "Input: Wow! That was hilarious! I'm laughing so hard.\n",
      "Predicted emotions: ['admiration', 'amusement']\n",
      "--------------------------------------------------\n",
      "Input: I'm proud of what we achieved together.\n",
      "Predicted emotions: ['admiration', 'amusement']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test multiple sentences\n",
    "sentences = [\n",
    "    \"I'm so happy and excited to see you.\",\n",
    "    \"Why did you do that? I'm really angry and confused.\",\n",
    "    \"I can't believe this is happening. I'm so disappointed.\",\n",
    "    \"Wow! That was hilarious! I'm laughing so hard.\",\n",
    "    \"I'm proud of what we achieved together.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    emotions = predict_emotions(sentence, loaded_model, loaded_tokenizer)\n",
    "    print(f'Input: {sentence}')\n",
    "    print(f'Predicted emotions: {emotions}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\youtube_automation\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "api_key = \"AIzaSyAGOnKsiRkWyikky3x9q2NRQRPPqaiIZ2I\"\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# List the models available to your API key\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mlist_models()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print model details\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.generativeai' has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "# List the models available to your API key\n",
    "models = genai.models.list_models()\n",
    "\n",
    "# Print model details\n",
    "for model in models:\n",
    "    print(f\"Model ID: {model.model_id}, Display Name: {model.display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'generate_message'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Generate a message using the 'text-bison-001' model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_message\u001b[49m(\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/text-bison-001\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the future of AI?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print the result\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidates\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.generativeai' has no attribute 'generate_message'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\RAJKUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\RAJKUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement google-ai (from versions: none)\n",
      "ERROR: No matching distribution found for google-ai\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-generativeai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-generativeai) (2.20.0)\n",
      "Requirement already satisfied: google-api-python-client in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-generativeai) (2.147.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-generativeai) (2.35.0)\n",
      "Requirement already satisfied: protobuf in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-generativeai) (5.28.2)\n",
      "Requirement already satisfied: pydantic in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-generativeai) (2.9.2)\n",
      "Requirement already satisfied: tqdm in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rajkumar\\appdata\\roaming\\python\\python310\\site-packages (from google-generativeai) (4.11.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-api-core->google-generativeai) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from pydantic->google-generativeai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajkumar\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.66.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.66.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\conda\\envs\\youtube_automation\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube_automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
